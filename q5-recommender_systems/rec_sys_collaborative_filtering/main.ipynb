{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0228f5e",
   "metadata": {},
   "source": [
    "# Recommender Systems Using Collaborative Filtering\n",
    "\n",
    "> *Recommender Systems*  \n",
    "> *MSc in Data Science, Department of Informatics*  \n",
    "> *Athens University of Economics and Business*\n",
    "\n",
    "---\n",
    "\n",
    "Find a ***rating-based*** or ***matching-based*** dataset that can be used to inform a recommender system based on ***collaborative filtering***.\n",
    "\n",
    "Build a Python notebook that:\n",
    "\n",
    "- Loads the dataset\n",
    "- Tries at least 2 different recommendations methods based on collaborative filtering (e.g., Count-based, Matrix Factorization, Tensorflow)\n",
    "- Uses quantitative metrics to evaluate the recommendations of each of the methods that you selected\n",
    "\n",
    "\n",
    "## *Table of Contents*\n",
    "\n",
    "- [*1. Introduction*](#introduction)\n",
    "    - [*1.1. Libraries*](#libraries)\n",
    "    - [*1.2. Data*](#data)\n",
    "    - [*1.3. Data Preprocessing*](#data_preprocessing)\n",
    "- [*2. Recommendations Using Item-Based Technique*](#item_based_technique)\n",
    "    - [*2.1. Loading User Ratings*](#loading_data)\n",
    "    - [*2.2. Create LSH Indices*](#lsh_indices)\n",
    "    - [*2.3. Make Recommendations*](#item_based_recommendations)\n",
    "    - [*2.4. Evaluate Recommendations Using Decision Support Methods*](#decision_support_methods)\n",
    "        - [*2.4.1. Precision*](#precision)\n",
    "        - [*2.4.2. Recall*](#recall)\n",
    "    - [*2.5. Evaluate Recommendations Using Ranking Based Methods*](#ranking_based_methods)\n",
    "        - [*2.5.1. nDCG*](#ndcg)\n",
    "        - [*2.5.2. Mean Reciprocal Rank*](#mrr)\n",
    "        - [*2.5.3. Average Precision*](#ap)\n",
    "- [*3. Recommendations Using Matrix Factorization (SVD)*](#matrix_factorization)\n",
    "    - [*3.1. Loading Data with Surprise Library*](#loading_data_with_surprise)\n",
    "    - [*3.2. Selecting Number of Factors*](#n_factors)\n",
    "    - [*3.3. Train SVD Algorithm*](#train_svd)\n",
    "    - [*3.4. Make Recommendations*](#svd_recommendations)\n",
    "    - [*3.5. Evaluate Recommendations*](#svd_evaluation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c96737",
   "metadata": {},
   "source": [
    "## Introduction <a class='anchor' id='introduction'></a>\n",
    "\n",
    "### *Libraries* <a class='anchor' id='libraries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8ac94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection.search import GridSearchCV\n",
    "\n",
    "from functions.data_preprocessing import preprocess_ratings_dataset\n",
    "from functions.data_preprocessing import preprocess_jokes_dataset\n",
    "from functions.data_preprocessing import unpivot_ratings\n",
    "\n",
    "from functions.item_based_recommendations import discretize_rating\n",
    "from functions.item_based_recommendations import load_ratings\n",
    "from functions.item_based_recommendations import create_LSH_index\n",
    "from functions.item_based_recommendations import get_neighbors\n",
    "from functions.item_based_recommendations import make_recommendations_using_item_based_technique\n",
    "from functions.item_based_recommendations import evaluate_recommendations_using_decision_support_methods\n",
    "from functions.item_based_recommendations import evaluate_recommendations_using_nDCG\n",
    "from functions.item_based_recommendations import evaluate_recommendations_using_MRR\n",
    "from functions.item_based_recommendations import evaluate_recommendations_using_AP\n",
    "\n",
    "from functions.matrix_factorization_recommendations import make_recommendations_using_matrix_factorization\n",
    "from functions.matrix_factorization_recommendations import evaluate_recommendations_by_matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ad8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66823ad7",
   "metadata": {},
   "source": [
    "### *Data* <a class='anchor' id='data'></a>\n",
    "\n",
    "- *The dataset that will be used is the [Jester Dataset for Recommender Systems and Collaborative Filtering Research](https://eigentaste.berkeley.edu/dataset/)*\n",
    "- *The dataset contains ratings for 150 jokes from over 50K users*\n",
    "- *You can see and rate some jokes by clicking [here](https://eigentaste.berkeley.edu/)*\n",
    "- *You can find more information about Jester by clicking [here](https://eigentaste.berkeley.edu/about.html)*\n",
    "\n",
    "##### *Read ratings data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f93e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ratings.shape: (54905, 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.28125</td>\n",
       "      <td>-9.28125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>-9.65625</td>\n",
       "      <td>-9.03125</td>\n",
       "      <td>-7.46875</td>\n",
       "      <td>-8.71875</td>\n",
       "      <td>-9.15625</td>\n",
       "      <td>-7.18750</td>\n",
       "      <td>-8.78125</td>\n",
       "      <td>-8.53125</td>\n",
       "      <td>-7.90625</td>\n",
       "      <td>-7.46875</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>9.90625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>8.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>8.68750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.12500</td>\n",
       "      <td>8.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.3125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.81250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>3.62500</td>\n",
       "      <td>9.3125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2.9375</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.15625</td>\n",
       "      <td>2.03125</td>\n",
       "      <td>5.6875</td>\n",
       "      <td>9.65625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.6875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.34375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.68750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.93750</td>\n",
       "      <td>9.53125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.93750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>3.71875</td>\n",
       "      <td>9.65625</td>\n",
       "      <td>-2.68750</td>\n",
       "      <td>-9.56250</td>\n",
       "      <td>-9.12500</td>\n",
       "      <td>9.84375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.93750</td>\n",
       "      <td>9.78125</td>\n",
       "      <td>9.81250</td>\n",
       "      <td>9.90625</td>\n",
       "      <td>3.12500</td>\n",
       "      <td>5.50000</td>\n",
       "      <td>-4.25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.12500</td>\n",
       "      <td>9.84375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.28125</td>\n",
       "      <td>4.96875</td>\n",
       "      <td>9.93750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>-4.3125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.78125</td>\n",
       "      <td>4.75000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.68750</td>\n",
       "      <td>-2.12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.31250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.93750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.56250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.84375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.84375</td>\n",
       "      <td>-7.21875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.03125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.93750</td>\n",
       "      <td>-9.96875</td>\n",
       "      <td>-9.87500</td>\n",
       "      <td>-9.81250</td>\n",
       "      <td>-9.78125</td>\n",
       "      <td>-6.84375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.8125</td>\n",
       "      <td>-9.78125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.8125</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.81250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.90625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.75000</td>\n",
       "      <td>-5.90625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.40625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.03125</td>\n",
       "      <td>3.87500</td>\n",
       "      <td>6.21875</td>\n",
       "      <td>5.65625</td>\n",
       "      <td>6.09375</td>\n",
       "      <td>5.40625</td>\n",
       "      <td>6.37500</td>\n",
       "      <td>7.03125</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>4.18750</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>2.28125</td>\n",
       "      <td>1.18750</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>3.21875</td>\n",
       "      <td>4.90625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53125</td>\n",
       "      <td>3.81250</td>\n",
       "      <td>4.46875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8125</td>\n",
       "      <td>4.8125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.4375</td>\n",
       "      <td>5.5625</td>\n",
       "      <td>6.09375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.96875</td>\n",
       "      <td>5.78125</td>\n",
       "      <td>6.25000</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>4.28125</td>\n",
       "      <td>1.96875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0625</td>\n",
       "      <td>2.59375</td>\n",
       "      <td>1.1875</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>2.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.8125</td>\n",
       "      <td>1.68750</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.90625</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>3.5625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0625</td>\n",
       "      <td>0.875</td>\n",
       "      <td>4.8125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.4375</td>\n",
       "      <td>3.90625</td>\n",
       "      <td>4.15625</td>\n",
       "      <td>2.87500</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>3.1875</td>\n",
       "      <td>6.28125</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>3.56250</td>\n",
       "      <td>6.15625</td>\n",
       "      <td>4.3750</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.75</td>\n",
       "      <td>7.71875</td>\n",
       "      <td>3.90625</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.59375</td>\n",
       "      <td>-7.90625</td>\n",
       "      <td>3.81250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.31250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.09375</td>\n",
       "      <td>-0.40625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.21875</td>\n",
       "      <td>-9.43750</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>-9.15625</td>\n",
       "      <td>3.65625</td>\n",
       "      <td>-9.43750</td>\n",
       "      <td>7.90625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.06250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.06250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.96875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.40625</td>\n",
       "      <td>8.12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.43750</td>\n",
       "      <td>4.40625</td>\n",
       "      <td>-8.28125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.56250</td>\n",
       "      <td>-8.1250</td>\n",
       "      <td>-4.8125</td>\n",
       "      <td>3.84375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.40625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.09375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1    2    3    4        5    6        7        8    9    10   11   \\\n",
       "user_id                                                                      \n",
       "1        NaN  NaN  NaN  NaN  0.21875  NaN -9.28125 -9.28125  NaN  NaN  NaN   \n",
       "2        NaN  NaN  NaN  NaN -9.68750  NaN  9.93750  9.53125  NaN  NaN  NaN   \n",
       "3        NaN  NaN  NaN  NaN -9.84375  NaN -9.84375 -7.21875  NaN  NaN  NaN   \n",
       "4        NaN  NaN  NaN  NaN  6.90625  NaN  4.75000 -5.90625  NaN  NaN  NaN   \n",
       "5        NaN  NaN  NaN  NaN -0.03125  NaN -9.09375 -0.40625  NaN  NaN  NaN   \n",
       "\n",
       "         12       13   14       15       16       17       18       19   \\\n",
       "user_id                                                                   \n",
       "1        NaN -6.78125  NaN  0.87500 -9.65625 -9.03125 -7.46875 -8.71875   \n",
       "2        NaN  9.93750  NaN  0.40625  3.71875  9.65625 -2.68750 -9.56250   \n",
       "3        NaN -2.03125  NaN -9.93750 -9.96875 -9.87500 -9.81250 -9.78125   \n",
       "4        NaN -0.40625  NaN -4.03125  3.87500  6.21875  5.65625  6.09375   \n",
       "5        NaN  7.50000  NaN -7.21875 -9.43750  0.12500 -9.15625  3.65625   \n",
       "\n",
       "             20       21       22       23       24       25       26   \\\n",
       "user_id                                                                  \n",
       "1       -9.15625 -7.18750 -8.78125 -8.53125 -7.90625 -7.46875  0.03125   \n",
       "2       -9.12500  9.84375      NaN      NaN      NaN      NaN  9.93750   \n",
       "3       -6.84375      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4        5.40625  6.37500  7.03125  0.09375      NaN  0.90625  4.18750   \n",
       "5       -9.43750  7.90625      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "             27       28       29       30       31       32      33   \\\n",
       "user_id                                                                 \n",
       "1        8.78125      NaN  8.78125      NaN  8.78125  8.78125     NaN   \n",
       "2        9.78125  9.81250  9.90625  3.12500  5.50000 -4.25000     NaN   \n",
       "3            NaN      NaN      NaN      NaN      NaN      NaN -9.8125   \n",
       "4        0.43750  2.28125  1.18750  0.28125  3.21875  4.90625     NaN   \n",
       "5       -9.06250      NaN  6.06250      NaN  5.96875      NaN     NaN   \n",
       "\n",
       "             34       35       36      37      38      39    40      41   \\\n",
       "user_id                                                                    \n",
       "1       -0.25000  8.78125  8.78125     NaN     NaN     NaN   NaN     NaN   \n",
       "2            NaN  5.12500  9.84375     NaN     NaN     NaN   NaN     NaN   \n",
       "3       -9.78125      NaN      NaN -9.8125  0.0625  0.0000   NaN     NaN   \n",
       "4        0.53125  3.81250  4.46875     NaN  2.8125  4.8125  0.25  4.4375   \n",
       "5            NaN  8.40625  8.12500     NaN     NaN     NaN   NaN     NaN   \n",
       "\n",
       "            42       43   44       45       46       47       48       49   \\\n",
       "user_id                                                                      \n",
       "1        0.0625      NaN  NaN      NaN      NaN      NaN      NaN  0.06250   \n",
       "2           NaN      NaN  NaN      NaN      NaN      NaN  9.28125  4.96875   \n",
       "3           NaN      NaN  NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4        5.5625  6.09375  0.5  3.96875  5.78125  6.25000  0.34375  4.28125   \n",
       "5           NaN      NaN  NaN -9.43750  4.40625 -8.28125      NaN      NaN   \n",
       "\n",
       "             50      51      52       53      54       55    56   57   58   \\\n",
       "user_id                                                                      \n",
       "1        9.90625  0.0625  0.0625  8.78125  8.6875      NaN   NaN  NaN  NaN   \n",
       "2        9.93750     NaN     NaN  8.00000 -4.3125      NaN   NaN  NaN  NaN   \n",
       "3            NaN     NaN     NaN      NaN     NaN      NaN   NaN  NaN  NaN   \n",
       "4        1.96875     NaN  1.0625  2.59375  1.1875  0.78125  2.75  NaN  NaN   \n",
       "5       -5.56250 -8.1250 -4.8125  3.84375     NaN      NaN   NaN  NaN  NaN   \n",
       "\n",
       "          59      60       61       62       63   64       65       66   67   \\\n",
       "user_id                                                                        \n",
       "1         NaN     NaN  0.03125  0.21875      NaN  NaN  8.78125  8.68750  NaN   \n",
       "2         NaN     NaN -0.78125  4.75000      NaN  NaN  3.68750 -2.12500  NaN   \n",
       "3         NaN     NaN      NaN      NaN      NaN  NaN      NaN      NaN  NaN   \n",
       "4       -0.25  1.8125  1.68750  0.62500  0.15625  NaN  4.90625  0.46875  NaN   \n",
       "5         NaN     NaN      NaN      NaN      NaN  NaN -5.40625      NaN  NaN   \n",
       "\n",
       "             68      69       70   71       72      73   74   75      76   \\\n",
       "user_id                                                                     \n",
       "1       -0.12500  8.6875      NaN  NaN  8.78125     NaN  NaN  NaN  9.3125   \n",
       "2            NaN -5.0000      NaN  NaN  4.78125     NaN  NaN  NaN     NaN   \n",
       "3            NaN     NaN      NaN  NaN      NaN     NaN  NaN  NaN     NaN   \n",
       "4        0.78125  0.5625  0.40625  NaN  8.00000  3.5625  NaN  NaN  7.0625   \n",
       "5            NaN     NaN      NaN  NaN -7.50000     NaN  NaN  NaN     NaN   \n",
       "\n",
       "           77      78   79      80       81       82       83       84   \\\n",
       "user_id                                                                   \n",
       "1          NaN     NaN  NaN  0.0625  0.12500      NaN  8.78125      NaN   \n",
       "2          NaN     NaN  NaN     NaN -0.31250      NaN  3.93750      NaN   \n",
       "3          NaN     NaN  NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "4        0.875  4.8125  NaN -6.4375  3.90625  4.15625  2.87500 -0.03125   \n",
       "5          NaN     NaN  NaN     NaN      NaN      NaN -7.50000      NaN   \n",
       "\n",
       "             85       86      87       88       89    90       91       92   \\\n",
       "user_id                                                                       \n",
       "1            NaN      NaN  8.0000      NaN  9.81250   NaN  8.78125  3.62500   \n",
       "2            NaN      NaN     NaN      NaN  9.56250   NaN      NaN      NaN   \n",
       "3            NaN      NaN     NaN      NaN      NaN   NaN      NaN      NaN   \n",
       "4        0.46875  0.34375  3.1875  6.28125  6.00000 -0.25  3.56250  6.15625   \n",
       "5            NaN      NaN     NaN      NaN -5.09375   NaN      NaN      NaN   \n",
       "\n",
       "            93   94    95       96       97       98       99       100  \\\n",
       "user_id                                                                   \n",
       "1        9.3125  NaN   NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2           NaN  NaN   NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3           NaN  NaN   NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4        4.3750  2.5  5.75  7.71875  3.90625  0.84375  0.46875  0.59375   \n",
       "5           NaN  NaN   NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "             101      102  103     104   105      106      107     108  \\\n",
       "user_id                                                                  \n",
       "1            NaN  0.75000 -5.0  2.9375  2.00 -0.15625  2.03125  5.6875   \n",
       "2            NaN      NaN  NaN     NaN   NaN      NaN      NaN     NaN   \n",
       "3            NaN  0.34375  NaN     NaN  1.25      NaN      NaN     NaN   \n",
       "4       -7.90625  3.81250  NaN  1.4375   NaN      NaN      NaN     NaN   \n",
       "5            NaN      NaN  NaN     NaN   NaN      NaN      NaN     NaN   \n",
       "\n",
       "             109  110  111  112  113  114  115  116  117  118      119  \\\n",
       "user_id                                                                  \n",
       "1        9.65625  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  0.0  8.78125   \n",
       "2            NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN      NaN   \n",
       "3       -9.81250  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN      NaN   \n",
       "4        0.31250  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN      NaN   \n",
       "5            NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN      NaN   \n",
       "\n",
       "             120      121  122      123  124  125  126     127  128  129  130  \\\n",
       "user_id                                                                         \n",
       "1        8.78125  8.78125  NaN  8.78125  NaN  NaN  NaN  8.6875  0.0  NaN  NaN   \n",
       "2            NaN      NaN  NaN      NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN   \n",
       "3            NaN      NaN  NaN      NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN   \n",
       "4            NaN      NaN  NaN      NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN   \n",
       "5            NaN      NaN  NaN      NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN   \n",
       "\n",
       "         131  132  133      134  135  136  137  138  139  140  141  142  143  \\\n",
       "user_id                                                                        \n",
       "1        NaN  NaN  NaN  3.34375  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2        NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "3        NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "4        NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "5        NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "         144  145  146  147  148  149  150  \n",
       "user_id                                     \n",
       "1        NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2        NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3        NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4        NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "5        NaN  NaN  NaN  NaN  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read ratings data\n",
    "df_ratings = pd.read_excel('./data/dataset_3_ratings.xls', header=None, usecols=list(range(1,151)))\n",
    "\n",
    "# preprocess the dataset\n",
    "df_ratings = preprocess_ratings_dataset(df_ratings)\n",
    "\n",
    "# shape\n",
    "print(f'df_ratings.shape: {df_ratings.shape}')\n",
    "\n",
    "# preview\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b74ad0",
   "metadata": {},
   "source": [
    "##### *Read jokes data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98e2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jokes.shape: (150, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_id</th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A man visits the doctor. The doctor says \"I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This couple had an excellent relationship goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Q. What's 200 feet long and has 4 teeth?   A. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Q. What's the difference between a man and a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Q.\\tWhat's O. J. Simpson's Internet address?  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   joke_id                                               joke\n",
       "0        1  A man visits the doctor. The doctor says \"I ha...\n",
       "1        2  This couple had an excellent relationship goin...\n",
       "2        3  Q. What's 200 feet long and has 4 teeth?   A. ...\n",
       "3        4  Q. What's the difference between a man and a t...\n",
       "4        5  Q.\\tWhat's O. J. Simpson's Internet address?  ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read jokes data\n",
    "df_jokes = pd.read_excel('./data/dataset_3_jokes.xlsx', header=None, names=['joke'])\n",
    "\n",
    "# preprocess the dataset\n",
    "df_jokes = preprocess_jokes_dataset(df_jokes)\n",
    "\n",
    "# shape\n",
    "print(f'df_jokes.shape: {df_jokes.shape}')\n",
    "\n",
    "# preview\n",
    "df_jokes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060f53d",
   "metadata": {},
   "source": [
    "### *Data Preprocessing* <a class='anchor' id='data_preprocessing'></a>\n",
    "\n",
    "##### *Unpivot ratings data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ac65e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ratings_up.shape: (1842370, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>joke_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.21875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0.06250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>-8.71875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>9.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>8.68750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id joke_id   rating\n",
       "0        1       5  0.21875\n",
       "1        1      80  0.06250\n",
       "2        1      19 -8.71875\n",
       "3        1      76  9.31250\n",
       "4        1     127  8.68750"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpivot the dataset\n",
    "df_ratings_up = unpivot_ratings(df_ratings)\n",
    "\n",
    "# shape\n",
    "print(f'df_ratings_up.shape: {df_ratings_up.shape}')\n",
    "\n",
    "# preview\n",
    "df_ratings_up.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038c1e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recommendations Using Item-Based Technique <a class='anchor' id='item_based_technique'></a>\n",
    "\n",
    "### *Loading User Ratings* <a class='anchor' id='loading_data'></a>\n",
    "\n",
    "- *In this step, we will preprocess the `df_ratings_up` dataframe*\n",
    "- *In particular, we will transform its data into a dictionary format*\n",
    "- *The keys of the dictionary will correspond to each user ID, while the values will be arrays containing the movie IDs rated along with their polarity*\n",
    "\n",
    "##### *Preprocess `df_ratings_up`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f8bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = load_ratings(df_ratings_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df544117",
   "metadata": {},
   "source": [
    "### *Create LSH Indices* <a class='anchor' id='lsh_indices'></a>\n",
    "\n",
    "##### *Create an index for each entity (e.g. joke ID) using Locality Sensitive Hashing (LSH)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6f6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50 out of 140 entities indexed.\n",
      "100 out of 140 entities indexed.\n",
      "140 out of 140 entities indexed.\n"
     ]
    }
   ],
   "source": [
    "index, hashes = create_LSH_index(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10915cb",
   "metadata": {},
   "source": [
    "### *Make Recommendations* <a class='anchor' id='item_based_recommendations'></a>\n",
    "\n",
    "##### *Select a random user to recommend jokes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d57c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 100 # select a user to recommend jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850a81d",
   "metadata": {},
   "source": [
    "##### *Make recommendations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810df0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend jokes for the user provided\n",
    "to_recommend, already_rated = make_recommendations_using_item_based_technique(user_id,\n",
    "                                                                              df_ratings_up,\n",
    "                                                                              df_jokes,\n",
    "                                                                              ratings,\n",
    "                                                                              index,\n",
    "                                                                              hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acfb424",
   "metadata": {},
   "source": [
    "##### *View recommendations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241a8be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JokeID: 148 - Scaled Votes Score: 3.8155886\n",
      "===========================================\n",
      "Recently a teacher, a garbage collector, and a lawyer wound up together at the Pearly Gates. St. Peter informed them that in order to get into Heaven, they would each have to answer one question. St. Peter addressed the teacher and asked, \"What was the name of the ship that crashed into the iceberg? They just made a movie about it.\" The teacher answered quickly, \"That would be the Titanic.\" St. Peter let him through the gate. St. Peter turned to the garbage man and, figuring Heaven didn't really need all the odors that this guy would bring with him, decided to make the question a little harder: \"How many people died on the ship?\" Fortunately for him, the trash man had just seen the movie. \"1,228,\" he answered. \"That's right! You may enter.\" St. Peter turned to the lawyer: \"Name them.\"\n",
      "\n",
      "JokeID: 134 - Scaled Votes Score: 3.5929973\n",
      "===========================================\n",
      "An artist asked the gallery owner if there had been any interest in his paintings currently on display. \"I've got good news and bad news,\" the owner replied. \"The good news is that a gentleman inquired about your work and wondered if it would appreciate in value after your death. When I told him it would, he bought all fifteen of your paintings.\" \"That's wonderful!\" the artist exclaimed. \"What's the bad news?\" With concern, the gallery owner replied: \"The guy was your doctor.\"\n",
      "\n",
      "JokeID: 137 - Scaled Votes Score: 2.4998754\n",
      "===========================================\n",
      "Deep within a forest, a little turtle began to climb a tree. After hours of effort, he reached the top, jumped into the air waving his front legs and crashed to the ground. After recovering, he slowly climbed the tree again, jumped, and fell to the ground. The turtle tried again and again, while a couple of birds sitting on a branch watched his sad efforts. Finally, the female bird turned to her mate. \"Dear,\" she chirped, \"I think it's time to tell him he's adopted.\"\n",
      "\n",
      "JokeID: 149 - Scaled Votes Score: 2.2095066\n",
      "===========================================\n",
      "A little girl asked her father, \"Daddy? Do all fairy tales begin with 'Once Upon a Time'\" He replied, \"No, there is a whole series of fairy tales that begin with 'If elected I promise'\"\n",
      "\n",
      "JokeID: 150 - Scaled Votes Score: 1.9830165\n",
      "===========================================\n",
      "In an interview with David Letterman, Carter passed along an anecdote of a translation problem in Japan. Carter was speaking at a business lunch in Tokyo, where he decided to open his speech with a brief joke. He told the joke, then waited for the translator to announce the Japanese version. Even though the story was quite short, Carter was surprised by how quickly the interpreter was able to re-tell it. Even more impressive was the reaction from the crowd. Carter thought the story was cute, but not outright hilarious, yet the crowd broke right up. Carter was very flattered. After the speech, Carter wanted to meet the translator to ask him how he told the joke. Perhaps there is better way to tell the joke? When Carter asked how the joke had been told in Japanese, the translator responded, \"I told them, 'President Carter has told a very funny joke. Please laugh now.'\"\n",
      "\n",
      "JokeID: 147 - Scaled Votes Score: 1.9407506\n",
      "===========================================\n",
      "It was the day of the big sale. Rumors of the sale (and some advertising in the local paper) were the main reason for the long line that formed by 8:30, the store's opening time, in front of the store. A small man pushed his way to the front of the line, only to be pushed back, amid loud and colorful curses. On the man's second attempt, he was punched square in the jaw, and knocked around a bit, and then thrown to the end of the line again. As he got up the second time, he said to the person at the end of the line... \"That does it! If they hit me one more time, I won't open the store!\"\n",
      "\n",
      "JokeID: 132 - Scaled Votes Score: 1.7203385\n",
      "===========================================\n",
      "Mickey Mouse is having a nasty divorce with Minnie Mouse. Mickey spoke to the judge about the separation.\"I'm sorry Mickey, but I can't legally separate you two on the grounds that Minnie is mentally insane...\" Mickey replied, \"I didn't say she was mentally insane, I said that she's fucking Goofy!\"\n",
      "\n",
      "JokeID: 145 - Scaled Votes Score: 1.5480246\n",
      "===========================================\n",
      "A blonde, brunette, and a red head are all lined up to be shot to death by a firing squad. The brunette shouts, \"Tornado!\" and the riflemen turn around to see the tornado. It isn't there, and the brunette uses that time to escape. The red head yells, \"Lightning!\" and the riflemen again turn to see the disaster, yet there is no disaster and the red head escapes. The blonde yells, \"Fire!\" The riflemen do.\n",
      "\n",
      "JokeID: 131 - Scaled Votes Score: 1.4704109\n",
      "===========================================\n",
      "A guy had been feeling down for so long that he finally decided to seek the aid of a psychiatrist. He went there, lay on the couch, spilled his guts then waited for the profound wisdom of the psychiatrist to make him feel better.The psychiatrist asked me a few questions, took some notes then sat thinking in silence for a few minutes with a puzzled look on his face. He looked up with an expression of delight and said, \"I think your problem is low self-esteem. It is very common among losers.\"\n",
      "\n",
      "JokeID: 143 - Scaled Votes Score: 1.1403141\n",
      "===========================================\n",
      "A preist, a 12-year-old kid, and the smartest guy in the world are on a plane. The pilot screams, \"The plane is going down! You have to jump!\" He then grabs a parachute and jumps off, leaving only two more parachutes on the plane. The smartest guy in the world says, \"I have to go. I mean, I'm the smartest guy in the world!\" He grabs a parachute, and jumps. The priest then looks at the 12-year-old kid, and says, \"Go, my son. You have a long life to live.\" The kid calmly responds: \"Dude, chill. We'll be fine. The 'smartest guy in the world' took my backpack.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop through\n",
    "# joke ID, (joke, polarity, score)\n",
    "for jid,(j,p,s) in to_recommend.items():\n",
    "    print(f'JokeID: {jid} - Scaled Votes Score: {round(s,7)}')\n",
    "    print('='*43)\n",
    "    print(f'{j}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14750dc",
   "metadata": {},
   "source": [
    "### *Evaluate Recommendations Using Decision Support Methods* <a class='anchor' id='decision_support_methods'></a>\n",
    "\n",
    "- *Decision support metrics help to understand how much RecSys are useful in recommending the \"right\" items to users*\n",
    "- *In particular, they assist users to take better decision by choosing good items and avoiding bad items*\n",
    "- *Two of the most commonly used metrics are Precision and Recall*\n",
    "\n",
    "### *Precision* <a class='anchor' id='precision'></a>\n",
    "\n",
    "- *Precision is the number of selected items that are relevant*\n",
    "- *Suppose our RecSys selects 3 items to recommend to users out of which 2 are relevant, then precision will be 66%*\n",
    "- *Precision is about retrieving the best items to the user assuming that there are more useful items available then you want*\n",
    "\n",
    "### *Recall* <a class='anchor' id='recall'></a>\n",
    "\n",
    "- *The recall is the number of relevant items that are selected*\n",
    "- *Suppose there are 6 relevant items out of which recommender selects 2 relevant items, then recall will be 33%*\n",
    "- *The recall is about not missing useful items*\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./images/decision_support_methods.png\" alt=\"description of image\">\n",
    "</div>\n",
    "\n",
    "##### *Compute Precision and Recall*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a661e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 30% - (20/67)\n",
      "   Recall: 67% - (20/30)\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommendations_using_decision_support_methods(user_id, df_ratings_up, already_rated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f5ef2",
   "metadata": {},
   "source": [
    "### *Evaluate Recommendations Using Ranking Based Methods* <a class='anchor' id='ranking_based_methods'></a>\n",
    "\n",
    "<p style='text-align: justify;'><i>Methods we touched so far allow us to understand the overall performance of the results we get from the RecSys. But they provide no information on how the items were ordered. A model can have a good <b>Precision</b> or <b>Recall</b>, but if the top three items that it recommends are not relevant to the user, then the recommendation is not much useful. If the user has to scroll down to search for relevant items then what’s the point of recommendations in the first place? Even without the recommendation user can scroll to look for items of their liking. <b>Ranking based evaluation methods</b> assist us in understanding how suggested items are ordered in terms of their relevancy for the users. They help us to measure quality items ranking.</i></p>\n",
    "\n",
    "### *nDCG* <a class='anchor' id='ndcg'></a>\n",
    "\n",
    "<p style='text-align: justify;'><i>nDCG has three parts. First is <b>\"CG\"</b> which stands for <b>Cumulative Gains</b>. It deals with the fact that most relevant items are more useful than somewhat relevant items that are more useful than irrelevant items. It sums the items based on its relevancy, hence, the term cumulative. Suppose we are asked to score the items based on their relevancy as:</i></p>\n",
    "\n",
    "- *(P) Most relevant score = 2*\n",
    "- *(A) Somewhat relevant score = 1*\n",
    "- *(N) Least relevant score = 0*\n",
    "\n",
    "*If we are to sum these score we will get cumulative gain for the given items as follows:*\n",
    "\n",
    "$$ \\mathrm{CG_{p}} = \\sum_{i=1}^{p} rel_i $$\n",
    "\n",
    "| Items Ranking | Relevancy Score |\n",
    "| :-----------: | --------------- |\n",
    "| Joke 1 | 1 |\n",
    "| Joke 3 | 2 |\n",
    "| Joke 2 | 2 |\n",
    "| Joke 5 | 0 |\n",
    "| Joke 4 | 1 |\n",
    "| **CG =** | **6** |\n",
    "\n",
    "<p style='text-align: justify;'><i>But CG doesn’t account for the position of the items on the list. And, hence, changing the item's position won’t change the CG. This is where the second part of nDCG comes in to play i.e. \"D\".</i></p>\n",
    "\n",
    "<p style='text-align: justify;'><i><b>Discounted Cumulative Gain</b>, <b>DCG</b> for short, penalizes the items that appear lower in the list. A relevant item appearing at the end of the list is a result of a bad recommender system and hence that item should be discounted to indicate the bad performance of the model. To do so we divide the relevance score of items with the log of its rank on the list.</i></p>\n",
    "\n",
    "$$ \\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{rel_{i}}{\\log_{2}(i+1)} $$\n",
    "\n",
    "| Items Ranking | Relevancy Score |\n",
    "| :-----------: | --------------- |\n",
    "| Joke 1 | 1 |\n",
    "| Joke 3 | 2 |\n",
    "| Joke 2 | 2 |\n",
    "| Joke 5 | 0 |\n",
    "| Joke 4 | 1 |\n",
    "| **CG =** | **6** |\n",
    "| **DCG =** | **12.1** |\n",
    "\n",
    "<p style='text-align: justify;'><i>DCG helps with the ranking, but suppose we are comparing the different lists of the recommender. DCG for each of the lists will be different depending upon where the recommender places the items. What will be DCG for when the most relevant item was placed at 10th position on 20 items list of recommender verses DCG for when the somewhat relevant item was paced at 10th position on 11th item list. To normalize this, \"n\" of nDCG, the third part, comes in to play.</i></p>\n",
    "\n",
    "<p style='text-align: justify;'><i><b>nDCG</b> normalized the DCG values of the different number of the items lists. To do so we sort the item list by relevancy and calculate the DCG for that list. This will be the perfect DCG score as items are sorted by their relevancy score. We divide all DCG score of all the list we get by this perfect DCG to get the normalized score for that list. <b>nDCG</b> score ranges from 0 to 1, where 1 indicates perfect ranking of relevant items and 0 indicates no relevant items in the recommendation list.</i></p>\n",
    "\n",
    "$$ {\\mathrm {nDCG_{{p}}}}={\\frac {DCG_{{p}}}{IDCG_{{p}}}}={\\frac{12.1}{13.9}}=0.87 $$\n",
    "\n",
    "*where <b>IDCG</b> is ideal discounted cumulative gain,*\n",
    "\n",
    "$$ \\mathrm{IDCG_p} = \\sum_{i=1}^{|REL_p|} \\frac{rel_i}{\\log_2(i+1)} $$\n",
    "\n",
    "*and ${|REL_p|}$ represents the list of relevant documents (ordered by their relevance) in the corpus up to position p.*\n",
    "\n",
    "| Perfect Ranking | Relevancy Score |\n",
    "| :-----------: | --------------- |\n",
    "| Joke 3 | 2 |\n",
    "| Joke 2 | 2 |\n",
    "| Joke 1 | 1 |\n",
    "| Joke 4 | 1 |\n",
    "| Joke 5 | 0 |\n",
    "| **CG =** | **6** |\n",
    "| **IDCG =** | **13.9** |\n",
    "\n",
    "##### *Compute nDCG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f5703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.89\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommendations_using_nDCG(already_rated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a80949",
   "metadata": {},
   "source": [
    "### *Mean Reciprocal Rank* <a class='anchor' id='mrr'></a>\n",
    "\n",
    "<p style='text-align: justify;'><i>Mean Reciprocal Rank, <b>MRR</b> for short, focuses on where is the first relevant item in the recommended list. <b>MRR</b> for a list with the first relevant item at its third position will be greater than for a list with the first relevant item at 4th position. <b>MRR</b> takes the reciprocal of the relevant items’ position and sums them. If relevant items are on positions 2, 3 and 5 on an item list, <b>MRR</b> will be $\\frac{1/2 + 1/3 + 1/5}{3}$.</i></p>\n",
    "\n",
    "| Items Ranking | Relevant Items | Reciprocal Ranking |\n",
    "| :-----------: | :------------: | :----------------: |\n",
    "| Joke 1 | No | 0 |\n",
    "| Joke 3 | Yes | 1/2 |\n",
    "| Joke 2 | Yes | 1/3 |\n",
    "| Joke 5 | No | 0 |\n",
    "| Joke 4 | Yes | 1/5 |\n",
    "\n",
    "$$ \\mathrm{AP} = {\\frac{\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{5}}{3}} = 0.34 $$\n",
    "\n",
    "<p style='text-align: justify;'><i>Typically, a higher <b>MRR</b> score indicates better performance, where a score of 1.0 represents perfect accuracy (i.e., all relevant items are ranked first), while a score of 0.0 represents random guessing. In general, an <b>MRR</b> score above 0.2 is considered to be a good score, but it can vary depending on the specific task and dataset.</i></p>\n",
    "\n",
    "<p style='text-align: justify;'><i>It's important to keep in mind that the <b>MRR</b> score is just one evaluation metric for a recommender system, and it should be considered alongside other metrics such as precision and recall. It's also important to consider the specific requirements of the application and the preferences of the users to determine what constitutes good performance.</i></p>\n",
    "\n",
    "##### *Compute MRR*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6d41a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank: 0.07\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommendations_using_MRR(already_rated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201ba82",
   "metadata": {},
   "source": [
    "### *Average Precision* <a class='anchor' id='ap'></a>\n",
    "\n",
    "<p style='text-align: justify;'><i>Precision helps to understand the overall performance of the model but doesn’t tell if the items were ranked properly. <b>Average Precision</b>, AP for short, helps to measure the quality of the selected item’s ranking of the recommender model. It calculates the precision for only the relevant items that are recommended.</i></p>\n",
    "\n",
    "<p style='text-align: justify;'><i>Suppose our model recommends 8 items, as depicted below, out of which 4 are correct and 4 are incorrect. We take the first relevant item and calculate its precision which in our case is the first item, therefore, its precision will be 1/1. Next, calculate precision for the second relevant item (item 3). Its precision will be 2/3. 2 because from 1st till the current item there are two correctly predicted items out of total 3 items. We will do the same for all the relevant items. Lastly, take the mean of the precision list to compute <b>AP</b>. The overall precision for this example is $0.5$, while the <b>AP</b> is $0.75$. Lower <b>AP</b> indicates the quality ranking. <b>AP</b> takes values from 0 (if there are no relevant recommendations) to 1 (if all recommendations are relevant).</i></p>\n",
    "\n",
    "<p style='text-align: justify;'><i><b>AP</b> gives more weight to the precision of the top recommendations, while precision gives equal weight to all the recommendations. This means that if the relevant items are mainly concentrated at the top of the recommendation list, <b>AP</b> will be higher than precision. On the other hand, if the relevant items are distributed randomly throughout the recommendation list, <b>AP</b> may be lower than precision. Therefore, it's possible for the AP to be either higher or lower than the precision, depending on the order in which the recommendations are presented and the distribution of the relevant items within the list.</i></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./images/average_precision.png\" alt=\"description of image\">\n",
    "</div>\n",
    "\n",
    "$$ \\mathrm{AP} = {\\frac{\\frac{1}{1} + \\frac{2}{3} + \\frac{3}{4} + \\frac{4}{7}}{4}} = 0.75 $$\n",
    "\n",
    "##### *Compute AP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3de38826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 37%\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommendations_using_AP(already_rated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fa9be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recommendations Using Matrix Factorization <a class='anchor' id='matrix_factorization'></a>\n",
    "\n",
    "### *Loading Data Using `Surprise` Library* <a class='anchor' id='loading_data_with_surprise'></a>\n",
    "\n",
    "##### *Load preprocessed ratings data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e4056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Reader class\n",
    "reader = Reader(rating_scale=(-10, 10))\n",
    "\n",
    "# load ratings dataset with Dataset class\n",
    "data = Dataset.load_from_df(df_ratings_up[['user_id', 'joke_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb0ef0",
   "metadata": {},
   "source": [
    "### *Selecting Number of Factors* <a class='anchor' id='n_factors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb3f126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Best RMSE score: 4.071266526583293\n",
      "Best hyperparameters: {'n_factors': 200}\n",
      "\n",
      "Elapsed time: 281 secs.\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "st = time.time()\n",
    "\n",
    "# define the hyperparameters to search\n",
    "param_grid = {'n_factors': range(50,250,50)}\n",
    "\n",
    "# perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5)\n",
    "grid_search.fit(data)\n",
    "\n",
    "# get the best number of factors\n",
    "n_factors = grid_search.best_params['rmse']['n_factors']\n",
    "\n",
    "# end time\n",
    "et = time.time()\n",
    "\n",
    "# print\n",
    "print(f'     Best RMSE score: {grid_search.best_score[\"rmse\"]}')\n",
    "print(f'Best hyperparameters: {grid_search.best_params[\"rmse\"]}')\n",
    "print()\n",
    "print(f'Elapsed time: {round(et-st)} secs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aadb86",
   "metadata": {},
   "source": [
    "### *Train SVD Algorithm* <a class='anchor' id='svd'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7bc1aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fa89750a670>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the train set\n",
    "train_set = data.build_full_trainset()\n",
    "\n",
    "# initialize the SVD algo\n",
    "svd = SVD(n_factors=n_factors)\n",
    "\n",
    "# fit the SVD\n",
    "svd.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9a20b",
   "metadata": {},
   "source": [
    "### *Make Recommendations* <a class='anchor' id='svd_recommendations'></a>\n",
    "\n",
    "##### *Select a random user to recommend jokes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b494ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 100 # select a user to recommend jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f241e",
   "metadata": {},
   "source": [
    "##### *Make recommendations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3417626",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = make_recommendations_using_matrix_factorization(svd, user_id, df_ratings_up, df_jokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07306270",
   "metadata": {},
   "source": [
    "##### *View recommendations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "633a7d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JokeID: 143 - Predicted Rating: 6.0\n",
      "===================================\n",
      "A preist, a 12-year-old kid, and the smartest guy in the world are on a plane. The pilot screams, \"The plane is going down! You have to jump!\" He then grabs a parachute and jumps off, leaving only two more parachutes on the plane. The smartest guy in the world says, \"I have to go. I mean, I'm the smartest guy in the world!\" He grabs a parachute, and jumps. The priest then looks at the 12-year-old kid, and says, \"Go, my son. You have a long life to live.\" The kid calmly responds: \"Dude, chill. We'll be fine. The 'smartest guy in the world' took my backpack.\"\n",
      "\n",
      "JokeID: 148 - Predicted Rating: 5.9\n",
      "===================================\n",
      "Recently a teacher, a garbage collector, and a lawyer wound up together at the Pearly Gates. St. Peter informed them that in order to get into Heaven, they would each have to answer one question. St. Peter addressed the teacher and asked, \"What was the name of the ship that crashed into the iceberg? They just made a movie about it.\" The teacher answered quickly, \"That would be the Titanic.\" St. Peter let him through the gate. St. Peter turned to the garbage man and, figuring Heaven didn't really need all the odors that this guy would bring with him, decided to make the question a little harder: \"How many people died on the ship?\" Fortunately for him, the trash man had just seen the movie. \"1,228,\" he answered. \"That's right! You may enter.\" St. Peter turned to the lawyer: \"Name them.\"\n",
      "\n",
      "JokeID: 138 - Predicted Rating: 5.5\n",
      "===================================\n",
      "WASHINGTON (Reuters) - A tragic fire on Monday destroyed the personal library of President George W. Bush. Both of his books have been lost. Presidential spokesman Ari Fleischer said the president was devastated, as he had not finished coloring the second one.\n",
      "\n",
      "JokeID: 145 - Predicted Rating: 4.7\n",
      "===================================\n",
      "A blonde, brunette, and a red head are all lined up to be shot to death by a firing squad. The brunette shouts, \"Tornado!\" and the riflemen turn around to see the tornado. It isn't there, and the brunette uses that time to escape. The red head yells, \"Lightning!\" and the riflemen again turn to see the disaster, yet there is no disaster and the red head escapes. The blonde yells, \"Fire!\" The riflemen do.\n",
      "\n",
      "JokeID: 134 - Predicted Rating: 4.2\n",
      "===================================\n",
      "An artist asked the gallery owner if there had been any interest in his paintings currently on display. \"I've got good news and bad news,\" the owner replied. \"The good news is that a gentleman inquired about your work and wondered if it would appreciate in value after your death. When I told him it would, he bought all fifteen of your paintings.\" \"That's wonderful!\" the artist exclaimed. \"What's the bad news?\" With concern, the gallery owner replied: \"The guy was your doctor.\"\n",
      "\n",
      "JokeID: 140 - Predicted Rating: 3.9\n",
      "===================================\n",
      "Chuck Norris' calendar goes straight from March 31st to April 2nd; no one fools Chuck Norris.\n",
      "\n",
      "JokeID: 139 - Predicted Rating: 3.4\n",
      "===================================\n",
      "In a Veteran's Day speech, President Bush vowed, We will finish the mission. Period. Afterwards, he was advised that he doesn't have to read the punctuation marks.\n",
      "\n",
      "JokeID: 135 - Predicted Rating: 2.9\n",
      "===================================\n",
      "A guy walked past a mental hospital and heard a moaning voice: \"13...13...13...13...\" The man looked over to the hospital and saw a hole in the wall. He looked through the hole and got poked in the eye. The moaning voice then groaned: \"14...14...14...14...\"\n",
      "\n",
      "JokeID: 132 - Predicted Rating: 2.7\n",
      "===================================\n",
      "Mickey Mouse is having a nasty divorce with Minnie Mouse. Mickey spoke to the judge about the separation.\"I'm sorry Mickey, but I can't legally separate you two on the grounds that Minnie is mentally insane...\" Mickey replied, \"I didn't say she was mentally insane, I said that she's fucking Goofy!\"\n",
      "\n",
      "JokeID: 150 - Predicted Rating: 2.5\n",
      "===================================\n",
      "In an interview with David Letterman, Carter passed along an anecdote of a translation problem in Japan. Carter was speaking at a business lunch in Tokyo, where he decided to open his speech with a brief joke. He told the joke, then waited for the translator to announce the Japanese version. Even though the story was quite short, Carter was surprised by how quickly the interpreter was able to re-tell it. Even more impressive was the reaction from the crowd. Carter thought the story was cute, but not outright hilarious, yet the crowd broke right up. Carter was very flattered. After the speech, Carter wanted to meet the translator to ask him how he told the joke. Perhaps there is better way to tell the joke? When Carter asked how the joke had been told in Japanese, the translator responded, \"I told them, 'President Carter has told a very funny joke. Please laugh now.'\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'JokeID: {recommendations.iloc[i,0]} - Predicted Rating: {round(recommendations.iloc[i,2],1)}')\n",
    "    print('===================================')\n",
    "    print(f'{recommendations.iloc[i,1]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382551d",
   "metadata": {},
   "source": [
    "### *Evaluate Recommendations* <a class='anchor' id='svd_evaluation'></a>\n",
    "\n",
    "##### *Compute RMSE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9dea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.34\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommendations_by_matrix_factorization(svd, user_id, df_ratings_up, df_jokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e643e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Thank you!*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
