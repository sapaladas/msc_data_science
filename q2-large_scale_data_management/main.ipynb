{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63833b6",
   "metadata": {},
   "source": [
    "# Using Apache Spark to Execute Queries \n",
    "\n",
    "> *Large Scale Data Management*  \n",
    "> *MSc in Data Science, Department of Informatics*  \n",
    "> *Athens University of Economics and Business*\n",
    "\n",
    "---\n",
    "\n",
    "For the final project you will use <b>Apache Spark</b> to execute queries on datasets.<br>\n",
    "There are 2 basic APIs for query execution, the <a href=\"https://spark.apache.org/docs/2.4.4/rdd-programming-guide.html\">RDD API</a> and the <a href=\"https://spark.apache.org/docs/2.4.4/sql-programming-guide.html\">DataFrame/SQL API</a>.<br>\n",
    "  To download the dataset you need to execute the following commands in your master machine:\n",
    "\n",
    "- `wget https://www.dropbox.com/s/yprrbtqhy0fi6os/datasets.tar.gz?dl=0`\n",
    "- `mv datasets.tar.gz?dl=0 datasets.tar.gz`\n",
    "- `tar -xzf datasets.tar.gz`\n",
    "\n",
    "The files are in csv format and executing queries in this format is not efficient.<br>\n",
    "To optimize data access, databases traditionally load data into a specific designed binary formats.<br>\n",
    "Spark has a similar approach too and we can convert datasets to a special format named \"parquet\".<br>\n",
    "The parquet file format has major benefits:\n",
    "\n",
    "1. *Has smaller footprint in memory and disk and therefore optimizes I/O, reducing execution time*\n",
    "2. *Maintains additional information, such as statistics on the dataset, which helps on more efficient processing*\n",
    "\n",
    "The parquet file format is a columnar file format, you can read more about it <a href=\"https://parquet.apache.org/\">here</a>.<br>\n",
    "On how to read and write parquet files you can find information <a href=\"https://spark.apache.org/docs/2.4.4/sql-data-sources-parquet.html\">here</a>.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288ba01",
   "metadata": {},
   "source": [
    "### *Task 1*\n",
    "\n",
    "- Create a files directory on HDFS and then upload the csv files to the files directory\n",
    "- Provide the commands needed for the directory creation and the files upload\n",
    "- Provide a print screen that shows the csv files in the directory you created\n",
    "- Convert the files to parquet and upload them on HDFS as well\n",
    "\n",
    "##### *Solution*\n",
    "\n",
    "- `./hdfs dfs -mkdir /lsdm_files`\n",
    "- `./hdfs dfs -mv /departmentsR.csv /lsdm_files/departmentsR.csv`\n",
    "- `./hdfs dfs -mv /departmentsR.csv /lsdm_files/employeesR.csv`\n",
    "- `./hdfs dfs -mv /departmentsR.csv /lsdm_files/movie_genres.csv`\n",
    "- `./hdfs dfs -mv /departmentsR.csv /lsdm_files/movies.csv`\n",
    "- `./hdfs dfs -mv /departmentsR.csv /lsdm_files/ratings.csv`\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/hdfs.png\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ffeb1a",
   "metadata": {},
   "source": [
    "### *Task 2*\n",
    "\n",
    "- Using RDDs write code to answer the following queries (Q1-Q5)\n",
    "- You can use the csv or parquet files you uploaded on HDFS\n",
    "\n",
    "##### *Solution*\n",
    "\n",
    "- The queries and the code can be found <a href=\"\">here</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8bc8e",
   "metadata": {},
   "source": [
    "### *Task 3*\n",
    "\n",
    "- Using DataFrames write code to answer the following queries (Q1-Q5)\n",
    "- You should use the parquet files you created and uploaded to HDFS\n",
    "\n",
    "##### *Solution*\n",
    "\n",
    "- The queries and the code can be found <a href=\"\">here</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab83f1",
   "metadata": {},
   "source": [
    "### *Task 4*\n",
    "\n",
    "- Using Spark SQL write code to answer the following queries (Q1-Q5)\n",
    "- You should use both csv and parquet files you created and uploaded to HDFS\n",
    "\n",
    "##### *Solution*\n",
    "\n",
    "- The queries and the code can be found <a href=\"\">here</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe466384",
   "metadata": {},
   "source": [
    "### *Task 5*\n",
    "\n",
    "- For every query (Q1-Q5) measure the execution time of each scenario:\n",
    "    1. Map/Reduce â€“ RDD API\n",
    "    2. Spark SQL on csv files\n",
    "    3. Spark SQL on parquet files\n",
    "- Create a bar chart with the execution times grouped by query number and write a paragraph explaining the results\n",
    "\n",
    "##### *Solution*\n",
    "\n",
    "<left><img src=\"./images/execution_times_table.png\"/></left>\n",
    "<center><img src=\"./images/execution_times_graph.png\"/></center>\n",
    "\n",
    "##### *Comment*\n",
    "\n",
    "<p style='text-align: justify;'>According to the graph, it is obvious that we have a clear winner in terms of execution times. Using Spark SQL on parquet files to execute the queries seems to be the fastest way for someone to proceed with. On the other hand, MapReduce using the RDD API seems to be by far the slowest and less efficient choice.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38093e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Thank you!*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
