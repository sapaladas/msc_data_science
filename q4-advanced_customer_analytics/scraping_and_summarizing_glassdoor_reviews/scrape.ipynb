{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb069e0",
   "metadata": {},
   "source": [
    "# Scraping Glassdoor Reviews\n",
    "\n",
    "> *Advanced Customer Analytics*  \n",
    "> *MSc in Data Science, Department of Informatics*  \n",
    "> *Athens University of Economics and Business*\n",
    "\n",
    "---\n",
    "\n",
    "<p style='text-align: justify;'>Select an English-speaking website that hosts customer reviews on products (or services, businesses, movies, events, etc.). Make sure that the website includes a free-text search box that users can use to search for products. Create a first Python notebook with a function called <code>scrape()</code>. The function should accept as a parameter a query (a word or short phrase). The function should then use <b><i>selenium</i></b> to (1) submit the query to the website's search box and retrieve the list of matching products, and (2) access the first product on the. list and download all its reviews into a csv file. For each review, the function should get the text, the rating and the date. One line per review, 3 fields per line.</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fd444",
   "metadata": {},
   "source": [
    "##### *Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc77a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39016e6",
   "metadata": {},
   "source": [
    "##### *Define a function to scrape Glassdoor reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a1fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(query:str, delay_time:int=5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        The job title for which reviews will be scraped from the Amazon page.\n",
    "    delay_time: int\n",
    "        Delay time between commands\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # open a new csv writer\n",
    "    # to store the information that will be scraped\n",
    "    fw = open('glassdoor_reviews.csv', 'w', encoding='utf8')\n",
    "    writer = csv.writer(fw, lineterminator='\\n')\n",
    "    writer.writerow(['text','rating','date'])\n",
    "    \n",
    "    # initialize the url in which we will search for reviews\n",
    "    url = 'https://www.glassdoor.com/Reviews/Amazon-Reviews-E6036.htm'\n",
    "    \n",
    "    # wear a mask to switch user agent\n",
    "    opts = Options()\n",
    "    opts.add_argument('Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36')\n",
    "    \n",
    "    # create a webdriver instance\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    \n",
    "    # visit the reviews url\n",
    "    driver.get(url)\n",
    "    \n",
    "    # wait 5s\n",
    "    time.sleep(delay_time)\n",
    "    \n",
    "    # accept cookies\n",
    "    cookies_button = driver.find_element(by=By.CSS_SELECTOR, value='[id=\"onetrust-accept-btn-handler\"]')\n",
    "    cookies_button.click()\n",
    "    \n",
    "    # insert the query\n",
    "    search_job_titles_field = driver.find_element(by=By.CSS_SELECTOR, value='[data-test=\"ContentFiltersJobTitleAC\"]')\n",
    "    search_job_titles_field.send_keys(query)\n",
    "    \n",
    "    # click the find reviews button\n",
    "    find_reviews_button = driver.find_element(by=By.CSS_SELECTOR, value='[data-test=\"ContentFiltersFindBtn\"]')\n",
    "    find_reviews_button.click()\n",
    "    \n",
    "    input() # wait until we sign in\n",
    "    \n",
    "    page_counter = 1 # to keep track of page count\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        print(f'Page {page_counter}')\n",
    "        \n",
    "        # get all the reviews in the page\n",
    "        reviews = driver.find_elements(by=By.CSS_SELECTOR, value='[class=\"noBorder empReview cf pb-0 mb-0\"]')\n",
    "        \n",
    "        # loop through the reviews\n",
    "        for review in reviews:\n",
    "            \n",
    "            # initialize main column attributes\n",
    "            text, rating, date = 'NA', 'NA', 'NA'\n",
    "            \n",
    "            \"\"\"Pros\"\"\"\n",
    "            try: # to find the review pros box\n",
    "                pros_box = review.find_element(by=By.CSS_SELECTOR, value='[data-test=\"pros\"]')\n",
    "            except:\n",
    "                pros_box = None\n",
    "            # if box found, extract pros\n",
    "            if pros_box: pros = pros_box.text.strip()\n",
    "                \n",
    "            \"\"\"Cons\"\"\"\n",
    "            try: # to find the review cons box\n",
    "                cons_box = review.find_element(by=By.CSS_SELECTOR, value='[data-test=\"cons\"]')\n",
    "            except:\n",
    "                cons_box = None\n",
    "            # if box found, extract cons\n",
    "            if cons_box: cons = cons_box.text.strip()\n",
    "                \n",
    "            \"\"\"Rating\"\"\"\n",
    "            try: # to find the review rating\n",
    "                rating_box = review.find_element(by=By.CSS_SELECTOR, value='[class=\"ratingNumber mr-xsm\"]')\n",
    "            except:\n",
    "                rating_box = None\n",
    "            # if box found, extract rating\n",
    "            if rating_box: rating = rating_box.text.split('.')[0].strip()\n",
    "                \n",
    "            \"\"\"Date\"\"\"\n",
    "            try: # to find the review date\n",
    "                date_box = review.find_element(by=By.CSS_SELECTOR, value='[class=\"middle common__EiReviewDetailsStyle__newGrey\"]')\n",
    "            except:\n",
    "                date_box = None\n",
    "            # if box found, extract date\n",
    "            if date_box: date = date_box.text.split('-')[0].strip()\n",
    "                \n",
    "            # concat pros and cons into a single text\n",
    "            text = pros + '.' + ' *separator* ' + cons + '.'\n",
    "            \n",
    "            # write a new row in the csv\n",
    "            writer.writerow([text, rating, date])\n",
    "            \n",
    "        # find the next page button\n",
    "        next_page_button = driver.find_element(by=By.CSS_SELECTOR, value='[data-test=\"pagination-next\"]')\n",
    "        \n",
    "        # check if it's the last page\n",
    "        if not next_page_button.is_enabled(): break\n",
    "        \n",
    "        # move to the next page\n",
    "        next_page_button.click()\n",
    "        \n",
    "        # increment counter\n",
    "        page_counter += 1\n",
    "        \n",
    "        # wait 5s\n",
    "        time.sleep(delay_time)\n",
    "        \n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6825364",
   "metadata": {},
   "source": [
    "##### *Execute function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40473fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Page 17\n",
      "Page 18\n",
      "Page 19\n",
      "Page 20\n",
      "Page 21\n",
      "Page 22\n",
      "Page 23\n",
      "Page 24\n"
     ]
    }
   ],
   "source": [
    "query = 'data scientist'\n",
    "scrape(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16029545",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Thank you!*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
