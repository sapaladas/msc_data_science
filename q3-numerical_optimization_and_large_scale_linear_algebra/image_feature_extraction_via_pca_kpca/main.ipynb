{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5a8b91",
   "metadata": {},
   "source": [
    "# Image Feature Extraction via PCA & kPCA\n",
    "\n",
    "> *Numerical Optimization and Large Scale Linear Algebra*  \n",
    "> *MSc in Data Science, Department of Informatics*  \n",
    "> *Athens University of Economics and Business*\n",
    "\n",
    "---\n",
    "\n",
    "<p style='text-align: justify;'>For this task, we use images from the Yale Face Database B, which contains $5760$ single light source gray-level images of $10$ subjects, each seen under $576$ viewing conditions. We take $51$ images of the first subject and $51$ images of the third subject as the training data, and $13$ images of each of them as testing data. Then all the images are aligned, and each image has $168 \\times 192$ pixels. We use the $168 \\times 192$ pixel intensities as the original features for each image, thus the original feature vector is $32256$-dimensional. Then we use standard PCA and Gaussian kernel PCA to extract the $9$ most significant features from the training data, and record the eigenvectors. For standard PCA, only the eigenvectors are needed to extract features from testing data. For Gaussian kernel PCA, both the eigenvectors and the training data are needed to extract features from testing data. Note that for standard PCA, there are particular fast algorithms to compute the eigenvectors when the dimensionality is much higher than the number of data points. For kernel PCA, we use a Gaussian kernel with $\\sigma = 22546$. For classification, we use the simplest linear classifier.</p>\n",
    "\n",
    "<p style='text-align: justify;'>Parameter selection for kernel PCA directly determines the performance of the algorithm. For Gaussian kernel PCA, the most important parameter is the $\\sigma$ in the kernel function defined by $\\kappa(x,y)=\\exp(-\\|x-y\\|^{2} \\div 2\\sigma^{2})$. The Gaussian kernel is a function of the distance $\\| x-y \\|$ between two vectors $x$ and $y$. Ideally, if we want to separate different classes in the new feature space, then the parameter $\\sigma$ shoud be smaller than inter-class distances, and larger than inner-class distances. However, we don't know how many classes are there in the data, thus it is not easy to estimate the inter-class or inner class distances. Alternatively, we can set $\\sigma$ to a small value to capture only the neighborhood information of each data point. For this purpose, for each data point $x_{i}$, let the distance from $x_{i}$ to its nearest neighbor be $d_{i}^{NN}$. In the experiments, we use this parameter selection strategy:<br><br>$$\\sigma=5mean(d_{i}^{NN}).$$<br>This strategy, in the experiments, ensures that the $\\sigma$ is large enough to capture neighboring data points, and is much smaller than inter-class distances. When using different datasets, this strategy may need modifications.</p>\n",
    "\n",
    "> ***Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models***  \n",
    "> *Quan Wang, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY 12180 USA*  \n",
    "> *arXiv:1207.3538 \\[cs.CV\\], 2012*\n",
    "\n",
    "## *Table of Contents*\n",
    "\n",
    "- [*1. Libraries*](#libraries)\n",
    "- [*2. Data*](#data)\n",
    "- [*3. PCA*](#pca)\n",
    "- [*4. Gaussian Kernel PCA*](#gaussian_kernel_pca)\n",
    "    - [*4.1. Manual Implementation*](#manual)\n",
    "    - [*4.2. Scikit Learn Implementation*](#sklearn)\n",
    "- [*5. Classification Results*](#classification_results)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a9de68",
   "metadata": {},
   "source": [
    "## Libraries <a class='anchor' id='libraries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f381a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6914a",
   "metadata": {},
   "source": [
    "## Data <a class='anchor' id='data'></a>\n",
    "\n",
    "##### *Define a function to read the data from the `.mat` file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccba363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (102, 32256)\n",
      "y_train.shape: (102, 1)\n",
      " x_test.shape: (26, 32256)\n",
      " y_test.shape: (26, 1)\n"
     ]
    }
   ],
   "source": [
    "# path and file of the data\n",
    "filename = './data/YaleFaceData.mat'\n",
    "\n",
    "def read_data(filename):\n",
    "    \n",
    "    # read .mat file\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    # define train and test set\n",
    "    x_train, y_train = f['train_x'][:].T, f['train_t'][:]\n",
    "    x_test, y_test = f['test_x'][:].T, f['test_t'][:]\n",
    "    \n",
    "    # shapes\n",
    "    print(f'x_train.shape: {x_train.shape}')\n",
    "    print(f'y_train.shape: {y_train.shape}')\n",
    "    print(f' x_test.shape: {x_test.shape}')\n",
    "    print(f' y_test.shape: {y_test.shape}')\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# execute function\n",
    "x_train, y_train, x_test, y_test = read_data(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9edaf",
   "metadata": {},
   "source": [
    "## PCA <a class='anchor' id='pca'></a>\n",
    "\n",
    "<p style='text-align: justify;'>The <b>principal components</b> of a collection of points in a real coordinate space are a sequence of $p$ unit vectors, where the $i$-th vector is the direction of a line that best fits the data while being orthogonal to the first $i-1$ vectors. Here, a best-fitting line is defined as one that minimizes the average squared distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. <b>Principal component analysis (PCA)</b> is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.</p>\n",
    "\n",
    "##### *Project train and test data onto a lower dimensional space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x_train, x_test, n_components=9):\n",
    "    \"\"\"\n",
    "    Principal Component Analysis\n",
    "    ____________________________\n",
    "    :param x_train: the input train data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :param x_test: the input unseen data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :param n_components: the number of principal components to keep\n",
    "    :return x_train_PCA: the projection of the input train data onto a lower dimensional space\n",
    "    :return x_test_PCA: the projection of the input unseen data onto a lower dimensional space\n",
    "    ============================\n",
    "    step1: initialize PCA instance\n",
    "    step2: fit PCA instance into the training set\n",
    "    step3: get the 9 largest eigenvectors as principal components\n",
    "    step3: project data onto a lower dimensional space\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize PCA instance\n",
    "    pca = PCA(n_components=n_components, random_state=1)\n",
    "    \n",
    "    # fit PCA instance\n",
    "    # into x_train\n",
    "    pca.fit(x_train)\n",
    "    \n",
    "    # get the 9 largest eigenvectors\n",
    "    # as principal components\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # project data onto a lower dimensional space\n",
    "    x_train_PCA = np.dot(x_train, eigenvectors.T)\n",
    "    x_test_PCA = np.dot(x_test, eigenvectors.T)\n",
    "    \n",
    "    return x_train_PCA, x_test_PCA\n",
    "\n",
    "# execute function\n",
    "x_train_PCA, x_test_PCA = pca(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d13508",
   "metadata": {},
   "source": [
    "## Gaussian Kernel PCA <a class='anchor' id='gaussian_kernel_pca'></a>\n",
    "\n",
    "<p style='text-align: justify;'>In the field of multivariate statistics, <b>kernel principal components analysis (kernel PCA)</b> is an extension of PCA that allows for the separability of non-linear data by making use of kernels. The basic idea behind it is to project the linearly inseparable data onto a higher dimensional space where it becomes linearly separable.</p>\n",
    "\n",
    "### *Manual Implementation* <a class='anchor' id='manual'></a>\n",
    "\n",
    "##### *Steps*\n",
    "\n",
    "1. Compute hyperparameter $\\sigma$\n",
    "2. Use Gaussian kernel PCA to project the train data onto a lower dimensional space and record the eigenvectors\n",
    "3. Use Gaussian kernel PCA and the eigenvectors obtained from previous step, to project the test data onto a lower dimensional space\n",
    "\n",
    "##### *Compute hyperparameter $\\sigma$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce99e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "σ = 22546\n"
     ]
    }
   ],
   "source": [
    "def compute_sigma(x):\n",
    "    \"\"\"\n",
    "    Hyperparameter σ\n",
    "    ________________\n",
    "    :param x: the input data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :return: the σ parameter\n",
    "    ================\n",
    "    step1: calculate the pairwise distances between all data points of the input matrix\n",
    "    step2: find the distance, be it di^NN, of each data point xi from its nearest neighbor\n",
    "    step3: calculate the mean of the smallest distances\n",
    "    step4: compute σ, σ = 5 x mean(di^NN)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate pairwise distances\n",
    "    distances = pairwise_distances(x)\n",
    "    distances = np.where(distances == 0, np.inf, distances)\n",
    "    \n",
    "    # find the distance of each data point from its nearest neighbor\n",
    "    min_distances = [min(dist) for dist in distances]\n",
    "    \n",
    "    # calculate the avg smallest distance\n",
    "    mean_distance = np.mean(min_distances)\n",
    "    \n",
    "    # compute sigma\n",
    "    sigma = 5 * mean_distance\n",
    "    \n",
    "    return sigma, print(f'σ = {int(sigma)}')\n",
    "\n",
    "# execute function\n",
    "sigma, _ = compute_sigma(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0801dfd",
   "metadata": {},
   "source": [
    "##### *Use Gaussian kernel PCA to project the train data onto a lower dimensional space and record the eigenvectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a53d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kPCA(x, sigma, n_components=9):\n",
    "    \"\"\"\n",
    "    Kernel PCA algorithm\n",
    "    ____________________\n",
    "    :param x: the input data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :param sigma: the hyperparameter σ\n",
    "    :param n_components: the number of principal components to keep\n",
    "    :return x_train_kPCA: the projection of the input data onto a lower dimensional space\n",
    "    :return eigenvectors: the principal [n_components] components\n",
    "    ====================\n",
    "    step1: construct the kernel matrix K0\n",
    "    step2: compute the gram matrix (aka center the kernel matrix) $K=K0-1_NK0-K01_N+1_NK01_N$\n",
    "    step3: compute the eigenvalues and the eigenvectors of $K^C$\n",
    "    step4: normalize the eigenvectors\n",
    "    step5: get the k first largest eigenvectors to become principal components\n",
    "    step6: project the input data onto a lower dimensional space\n",
    "    \"\"\"\n",
    "    \n",
    "    def rbf_kernel(x, sigma):\n",
    "        \"\"\"\n",
    "        Gaussian rbf kernel\n",
    "        ___________________\n",
    "        :param x: the input data matrix, each row is one observation (M), each column is one feature (N)\n",
    "        :param sigma: the hyperparameter σ\n",
    "        :return: the rbf kernel, $e^{-||x-y||^2/2*sigma^2}$\n",
    "        ===================\n",
    "        step1: calculate the squared euclidean distance for every pair of points in the MxN dimensional dataset\n",
    "        step2: convert the pairwise distances into a symmetric MxM matrix\n",
    "        step3: compute the MxM kernel matrix\n",
    "        \"\"\"\n",
    "        D = pdist(x,'sqeuclidean')\n",
    "        D = squareform(D)\n",
    "        D = np.where(D<0,0,D)\n",
    "        K = np.sqrt(D)\n",
    "        K = K**2\n",
    "        K = np.exp(-K/(2*sigma**2))\n",
    "        return K\n",
    "    \n",
    "    # construct the kernel matrix\n",
    "    K0 = rbf_kernel(x, sigma)\n",
    "    \n",
    "    # compute the gram matrix\n",
    "    N = x.shape[0]\n",
    "    oneN = np.ones((N,N))/N\n",
    "    K = K0 - np.dot(oneN,K0) - np.dot(K0,oneN) + np.dot(np.dot(oneN,K0),oneN)\n",
    "    \n",
    "    # eigenvalue analysis\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(K)\n",
    "    \n",
    "    # normalization\n",
    "    norm_eigenvectors = np.sqrt(sum(eigenvectors**2))\n",
    "    eigenvectors = eigenvectors / np.tile(norm_eigenvectors, (eigenvectors.shape[0], 1))\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    kLargestIdx = np.argsort(eigenvalues)[::-1][:n_components]\n",
    "    eigenvectors = eigenvectors[:,kLargestIdx]\n",
    "    x_train_kPCA = np.dot(K0,eigenvectors)\n",
    "    \n",
    "    return x_train_kPCA, eigenvectors\n",
    "\n",
    "# execute function\n",
    "x_train_kPCA, eigenvectors = kPCA(x_train, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43209560",
   "metadata": {},
   "source": [
    "##### *Use Gaussian kernel PCA and the eigenvectors obtained from previous step, to project the test data onto a lower dimensional space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac630f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kPCA_newData(x_train, x_test, eigenvectors, sigma):\n",
    "    \"\"\"\n",
    "    Kernel PCA algorithm for unseen data\n",
    "    ____________________________________\n",
    "    :param x_train: the input train data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :param x_test: the input unseen data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :param eigVector: the eigenvectors obtained from the previous step\n",
    "    :param sigma: the hyperparameter σ\n",
    "    :return x_test_kPCA: the projection of the input unseen data onto a lower dimensional space\n",
    "    ====================================\n",
    "    step1: construct the kernel matrix K \n",
    "    step2: compute the projection of the input data onto a lower dimensional space\n",
    "    \"\"\"\n",
    "    \n",
    "    def rbf_kernel_newData(x_train, x_test, sigma):\n",
    "        \"\"\"\n",
    "        Gaussian rbf kernel\n",
    "        ___________________\n",
    "        :param x_train: the input train data matrix, each row is one observation (M), each column is one feature (N)\n",
    "        :param x_test: the input unseen data matrix, each row is one observation (M), each column is one feature (N)\n",
    "        :param sigma: the hyperparameter σ\n",
    "        :return: the rbf kernel, $e^{-||x-y||^2/2*sigma^2}$\n",
    "        ===================\n",
    "        step1: calculate the squared euclidean distance for every pair of points in the MxN dimensional dataset\n",
    "        step2: convert the pairwise distances into a symmetric MxM matrix\n",
    "        step3: compute the MxM kernel matrix\n",
    "        \"\"\"\n",
    "        D = pdist(np.concatenate([x_train,x_test]), 'sqeuclidean')\n",
    "        D = squareform(D)\n",
    "        D = np.where(D<0,0,D)\n",
    "        K = np.sqrt(D)\n",
    "        N = x_train.shape[0]\n",
    "        K = K[N:,:N]\n",
    "        K = K**2\n",
    "        K = np.exp(-K/(2*sigma**2))\n",
    "        return K\n",
    "    \n",
    "    # construct the kernel matrix\n",
    "    K = rbf_kernel_newData(x_train, x_test, sigma)\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    x_test_kPCA = np.dot(K,eigenvectors)\n",
    "    \n",
    "    return x_test_kPCA\n",
    "\n",
    "# execute function\n",
    "x_test_kPCA = kPCA_newData(x_train, x_test, eigenvectors, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247d811",
   "metadata": {},
   "source": [
    "## *Scikit Learn Implementation*\n",
    "\n",
    "##### *Use the scikit-learn kernel PCA algorithm to project train and test data onto a lower dimensional space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f3f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kPCA_sklearn(x_train, x_test, sigma, n_components=9):\n",
    "    \"\"\"\n",
    "    Kernel PCA algorithm using scikit-learn implementation\n",
    "    ______________________________________________________\n",
    "    :param x_train: the input train data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :param x_test: the input unseen data matrix, each row is one observation (M), each column is one feature (N)\n",
    "    :param sigma: the hyperparameter σ\n",
    "    :param n_components: the number of principal components to keep\n",
    "    :return x_train_PCA_2: the projection of the input train data onto a lower dimensional space\n",
    "    :return x_test_PCA_2: the projection of the input unseen data onto a lower dimensional space\n",
    "    ======================================================\n",
    "    step1: initialize kernel PCA instance\n",
    "    step2: fit and project both input and unseen data onto a lower dimensional space\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize kernel PCA instance\n",
    "    kPCA = KernelPCA(n_components=n_components, kernel='rbf', gamma=(1/(2*sigma**2)))\n",
    "\n",
    "    # project train and test data in a lower dimension\n",
    "    x_train_kPCA_2 = kPCA.fit_transform(x_train)\n",
    "    x_test_kPCA_2 = kPCA.transform(x_test)\n",
    "    \n",
    "    return x_train_kPCA_2, x_test_kPCA_2\n",
    "\n",
    "# execute function\n",
    "x_train_kPCA_2, x_test_kPCA_2 = kPCA_sklearn(x_train, x_test, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe494d1f",
   "metadata": {},
   "source": [
    "## Classification Results <a class='anchor' id='classification_results'></a>\n",
    "\n",
    "##### *Comparing classification results between the two dimensionality reduction methods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba62912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PCA train error: 8.82%\n",
      "  PCA test error: 23.08%\n",
      "\n",
      "kPCA train error: 6.86% - (manual)\n",
      " kPCA test error: 11.54% - (manual)\n",
      "\n",
      "kPCA train error: 6.86% - (sklearn)\n",
      " kPCA test error: 11.54% - (sklearn)\n"
     ]
    }
   ],
   "source": [
    "def compute_error_rates(x_fit, y_fit, x_pred, y_true):\n",
    "    # initialize model\n",
    "    model = LinearRegression()\n",
    "    # fit the model in the data\n",
    "    model.fit(x_fit, y_fit.ravel())\n",
    "    # make predictions\n",
    "    predictions = model.predict(x_pred)\n",
    "    # translate prediction\n",
    "    predictions = np.where(predictions>0,1,-1)\n",
    "    # compute error rate\n",
    "    error_rate = 1 - accuracy_score(y_true, predictions)\n",
    "    return error_rate\n",
    "\n",
    "# PCA train\n",
    "error_rate = compute_error_rates(x_train_PCA, y_train, x_train_PCA, y_train)\n",
    "print(f' PCA train error: {round(error_rate*100,2)}%')\n",
    "\n",
    "# PCA test\n",
    "error_rate = compute_error_rates(x_train_PCA, y_train, x_test_PCA, y_test)\n",
    "print(f'  PCA test error: {round(error_rate*100,2)}%\\n')\n",
    "\n",
    "# Kernel PCA train (manual)\n",
    "error_rate = compute_error_rates(x_train_kPCA, y_train, x_train_kPCA, y_train)\n",
    "print(f'kPCA train error: {round(error_rate*100,2)}% - (manual)')\n",
    "\n",
    "# Kernel PCA test (manual)\n",
    "error_rate = compute_error_rates(x_train_kPCA, y_train, x_test_kPCA, y_test)\n",
    "print(f' kPCA test error: {round(error_rate*100,2)}% - (manual)\\n')\n",
    "\n",
    "# Kernel PCA train (sklearn)\n",
    "error_rate = compute_error_rates(x_train_kPCA_2, y_train, x_train_kPCA_2, y_train)\n",
    "print(f'kPCA train error: {round(error_rate*100,2)}% - (sklearn)')\n",
    "\n",
    "# Kernel PCA test (sklearn)\n",
    "error_rate = compute_error_rates(x_train_kPCA_2, y_train, x_test_kPCA_2, y_test)\n",
    "print(f' kPCA test error: {round(error_rate*100,2)}% - (sklearn)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ce0e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Thank you!*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
